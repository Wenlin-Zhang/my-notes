{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Recurrent Neural Networks using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be building a character-level language model to generate character sequences, a la Andrej Karpathy’s [char-rnn](https://github.com/karpathy/char-rnn) (and see, e.g., a Tensorflow implementation by Sherjil Ozair [here](https://github.com/sherjilozair/char-rnn-tensorflow))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN Model Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time step $t$, for $t \\in \\{0,1,…n\\}$ the model accepts a input vector $X_t$ and a previous state vector $S_{t−1}$ as inputs and produces a state vector $S_t$, and a predicted probability distribution vector $P_t$, for the output vector $Y_t$.\n",
    "\n",
    "Formally, the model is:\n",
    "$$\n",
    "S_t=\\textbf{tanh}(W([X_t, S_{t−1})]+b_s)\n",
    "$$\n",
    "$$\n",
    "P_t=\\textbf{softmax}(U S_t + b_p)\n",
    "$$\n",
    "where $[X_t, S_{t−1}]$ represents vector concatenation, $X_t \\in R^n$ is a input vector, $W \\in R^{d \\times (n+d)}$, $b_s \\in R^d$, $U \\in R^{n \\times d}$, $b_p \\in R^n$ and d is the size of the state vector (I use $d=4$ below). At time step 0, $S_{−1}$ (the initial state) is initialized as a vector of zeros.\n",
    "\n",
    "Here is a diagram of the model:\n",
    "\n",
    "![Basic RNN](.\\BasicRNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNNs in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from last notebook that we represented each duplicate tensor of our RNN (e.g., the rnn inputs, rnn outputs, the predictions and the loss) as a list of tensors:\n",
    "![Basic RNN Labeled](.\\BasicRNNLabeled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Different RNN cell type in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a feedforward DNN, we can swap out activation functions (e.g., change the tanh activation to a sigmoid or a relu activation). In RNN, we can aslo swap out the entire recurrence function (cell) in an RNN. This is possible because the RNN cells conform to a general structure: every RNN cell is a function of the current input, $X_t$, and the prior state, $S_{t−1}$, that outputs a current state, $S_t$, and a current output, $Y_t$. \n",
    "\n",
    "Note that while for basic RNN cells, the current output equals the current state ($Y_t=S_t$), this does not have to be the case. We’ll see how LSTMs and multi-layered RNNs diverge from this below.\n",
    "\n",
    "Two popular choices for RNN cells are the GRU cell and the LSTM cell. By using gates, GRU and LSTM cells avoid the vanishing gradient problem and allow the network to learn longer-term dependencies. Their internals are quite complicated, here is some references:\n",
    "\n",
    "- [1] [Understanding LSTM Networks, by Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [2] [Written Memories: Understanding, Deriving and Extending the LSTM, by R2RT](http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tensorflow, All we have to do to upgrade our vanilla RNN cell is to replace this line:\n",
    "\n",
    "> \n",
    "```\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "```\n",
    "\n",
    "with this for LSTM:\n",
    "\n",
    "> \n",
    "```\n",
    "cell = tf.nn.rnn_cell.LSTMCell(state_size)\n",
    "```\n",
    "\n",
    "or this for GRU:\n",
    "\n",
    "> \n",
    "```\n",
    "cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "```\n",
    "\n",
    "The LSTM keeps two sets of internal state vectors, $c$ (for memory cell or constant error carousel) and $h$ (for hidden state). By default, they are concatenated into a single vector, but as of this writing, using the default arguments to LSTMCell will produce a warning message:\n",
    "\n",
    "> \n",
    "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.LSTMCell object at 0x7faade1708d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
    "\n",
    "This error tells us that it’s faster to represent the LSTM state as a tuple of $c$ and $h$, rather than as a concatenation of $c$ and $h$. You can tack on the argument `state_is_tuple=True` to have it do that.\n",
    "\n",
    "By using a tuple for the state, we can also easily replace the base cell with a `MultiRNNCell` for multiple layers. To see why this works, consider that while a single cell:\n",
    "\n",
    "![Basic RNN cell](.\\RNN_BasicRNNCell.png)\n",
    "\n",
    "looks different from a two cells stacked on top of each other:\n",
    "\n",
    "![Multiple RNN cell](.\\RNN_MultiRNNCellUngrouped.png)\n",
    "\n",
    "we can wrap the two cells into a single two-layer cell to make them look and behave as a single cell:\n",
    "\n",
    "![Multiple RNN cell grouped](.\\RNN_MultiRNNCellGrouped.png)\n",
    "\n",
    "To make this switch, we call `tf.nn.rnn_cell.MultiRNNCell`, which takes a list of `RNNCells` as its inputs and wraps them into a single cell:\n",
    "\n",
    "> \n",
    "```\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.BasicRNNCell(state_size)] * num_layers)\n",
    "```\n",
    "\n",
    "Note that if you are wrapping an `LSTMCell` that uses `state_is_tuple=True`, you should pass this same argument to the `MultiRNNCell` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build basic RNN graph statically with list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_basic_rnn_graph_with_list(\n",
    "    state_size = 100,\n",
    "    num_classes = 1024,  # vocab_size\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    x_one_hot = tf.one_hot(x, num_classes)\n",
    "    rnn_inputs = [tf.squeeze(i,squeeze_dims=[1]) for i in tf.split(1, num_steps, x_one_hot)]\n",
    "\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "\n",
    "    y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]\n",
    "\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "    losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 11.47535490989685 seconds to build the graph.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "build_basic_rnn_graph_with_list()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build multilayer RNN graph statically with list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_graph_with_list(\n",
    "    state_size = 100,\n",
    "    num_classes = 1000, # vocab_size\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 3,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "    rnn_inputs = [tf.squeeze(i) for i in tf.split(1,\n",
    "                                num_steps, tf.nn.embedding_lookup(embeddings, x))]\n",
    "\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "\n",
    "    y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]\n",
    "\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "    losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 55.63835382461548 seconds to build the graph.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "build_multilayer_lstm_graph_with_list()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 More efficient RNN graph building ---- using `dynamic_rnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_graph_with_dynamic_rnn(\n",
    "    state_size = 100,\n",
    "    num_classes = 1000, # vocab_size\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 3,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    # Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y so we can get the logits in a single matmul\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.7402036190032959 seconds to build the graph.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "build_multilayer_lstm_graph_with_dynamic_rnn()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 More efficient RNN graph building ---- using `tf.scan `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s not a breeze to work through and understand the `dynamic_rnn` code (which lives [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py)), but we can obtain a similar result ourselves by using `tf.scan` (`dynamic_rnn` does not use scan). `Scan` runs just a tad slower than Tensorflow’s optimized code, but is easier to understand and write yourself.\n",
    "\n",
    "`Scan` is a higher-order function that you might be familiar with if you’ve done any programming in OCaml, Haskell or the like. In general, it takes a function ($f:(x_t,y_{t−1})↦y_t$), a sequence ($[x_0,x_1,\\cdots x_n]$) and an initial value ($y_{−1}$) and returns a sequence ($[y_0,y_1, \\cdots, y_n]$) according to the rule: $y_t=f(x_t,y_{t−1})$. In Tensorflow, `scan` treats the first dimension of a Tensor as the sequence. Thus, if fed a Tensor of shape `[n, m, o]` as the sequence, scan would unpack it into a sequence of `n`-tensors, each with shape `[m, o]`. You can learn more about Tensorflow’s `scan` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functional_ops.md#tfscanfn-elems-initializernone-parallel_iterations10-back_proptrue-swap_memoryfalse-namenone-scan).\n",
    "\n",
    "Below, I use scan with an LSTM so as to compare to the `dynamic_rnn` using Tensorflow above. Because LSTMs store their state in a 2-tuple, and we’re using a 3-layer network, the scan function produces, as final_states below, a 3-tuple (one for each layer) of 2-tuples (one for each LSTM state), each of shape `[num_steps, batch_size, state_size]`. We need only the last state, which is why we `unpack`, `slice` and `repack` `final_states` to get `final_state` below.\n",
    "\n",
    "Another thing to note is that `scan` produces `rnn_outputs` with shape `[num_steps, batch_size, state_size]`, whereas the `dynamic_rnn` produces `rnn_outputs` with shape `[batch_size, num_steps, state_size]` (the first two dimensions are switched). `dynamic_rnn` has the flexibility to switch this behavior, using the “`time_major`” argument. `Tf.scan` does not have this flexibility, which is why we transpose `rnn_inputs` and `y` in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_graph_with_scan(\n",
    "    state_size = 100,\n",
    "    num_classes = 1000, # vocab_size\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 3,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_states = \\\n",
    "        tf.scan(lambda a, x: cell(x, a[1]),\n",
    "                tf.transpose(rnn_inputs, [1,0,2]),\n",
    "                initializer=(tf.zeros([batch_size, state_size]), init_state))\n",
    "\n",
    "    # there may be a better way to do this:\n",
    "    final_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                  tf.squeeze(tf.slice(c, [num_steps-1,0,0], [1, batch_size, state_size])),\n",
    "                  tf.squeeze(tf.slice(h, [num_steps-1,0,0], [1, batch_size, state_size])))\n",
    "                       for c, h in final_states])\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(tf.transpose(y,[1,0]), [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.8013379573822021 seconds to build the graph.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "g = build_multilayer_lstm_graph_with_scan()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `scan` was only marginally slower than using `dynamic_rnn`, and gives us the flexibility and understanding to tweak the code if we ever need to (e.g., if for some reason we wanted to create a skip connection from the state at timestep $t-2$ to timestep $t$, it would be easy to do with scan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Data preparation and training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 1115394\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and process data, utility functions\n",
    "\"\"\"\n",
    "\n",
    "file_url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
    "file_name = 'tinyshakespeare.txt'\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(file_url, file_name)\n",
    "\n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))\n",
    "\n",
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "del raw_data\n",
    "\n",
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(g, num_epochs, num_steps = 200, batch_size = 32, verbose = True, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "\n",
    "                feed_dict={g['x']: X, g['y']: Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['init_state']] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final_state'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            training_losses.append(training_loss/steps)\n",
    "\n",
    "        if isinstance(save, str):\n",
    "            g['saver'].save(sess, save)\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with different RNNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-2b2c0811b0b4>:4 in train_network.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.models.rnn.ptb.reader' has no attribute 'ptb_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-733dfaceef6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_multilayer_lstm_graph_with_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"It took\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds to train for 3 epochs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2b2c0811b0b4>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(g, num_epochs, num_steps, batch_size, verbose, save)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtraining_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-adbd2dbf4c15>\u001b[0m in \u001b[0;36mgen_epochs\u001b[0;34m(n, num_steps, batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgen_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptb_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.models.rnn.ptb.reader' has no attribute 'ptb_iterator'"
     ]
    }
   ],
   "source": [
    "g = build_multilayer_lstm_graph_with_list(num_classes = vocab_size)\n",
    "t = time.time()\n",
    "train_network(g, 3)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 3 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_multilayer_lstm_graph_with_dynamic_rnn(num_classes = vocab_size)\n",
    "t = time.time()\n",
    "train_network(g, 3)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 3 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_multilayer_lstm_graph_with_scan(num_classes = vocab_size)\n",
    "t = time.time()\n",
    "train_network(g, 3)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 3 epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Advanced -- Adding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding features like dropout to the network is easy: we figure out where they belong and drop them in.\n",
    "\n",
    "Dropout belongs in between layers, not on the state or in intra-cell connections. See [Zaremba et al. (2015), Recurrent Neural Network Regularization](https://arxiv.org/pdf/1409.2329.pdf) (“The main idea is to apply the dropout operator only to the non-recurrent connections.”)\n",
    "\n",
    "Thus, to apply dropout, we need to wrap the input and/or output of each cell. In our RNN implementation using list, we might do something like this:\n",
    "\n",
    "> \n",
    "```\n",
    "rnn_inputs = [tf.nn.dropout(rnn_input, keep_prob) for rnn_input in rnn_inputs]\n",
    "rnn_outputs = [tf.nn.dropout(rnn_output, keep_prob) for nn_output in rnn_outputs]\n",
    "```\n",
    "\n",
    "In our `dynamic_rnn` or `scan` implementations, we might apply dropout directly to the `rnn_inputs` or `rnn_outputs`:\n",
    "\n",
    "> \n",
    "```\n",
    "rnn_inputs = tf.nn.dropout(rnn_inputd, keep_prob)\n",
    "rnn_outputs = tf.nn.dropout(rnn_outputd, keep_prob)\n",
    "```\n",
    "\n",
    "But what happens when we use `MultiRNNCell`? How can we have dropout in between layers like in Zaremba et al. (2015)? The answer is to wrap our base RNN cell with dropout, thereby including it as part of the base cell, similar to how we wrapped our three RNN cells into a single `MultiRNNCell` above. Tensorflow allows us to do this without writing a new RNNCell by using `tf.nn.rnn_cell.DropoutWrapper`:\n",
    "\n",
    "> \n",
    "```\n",
    "cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=input_dropout, output_keep_prob=output_dropout)\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "```\n",
    "\n",
    "Note that if we wrap a base cell with dropout and then use it to build a `MultiRNNCell`, both input dropout and output dropout will be applied between layers (so if both are, say, $0.9$, the dropout in between layers will be $0.9 \\times 0.9 = 0.81$). If we want equal dropout on all inputs and outputs of a multi-layered RNN, we can use only output or input dropout on the base cell, and then wrap the entire `MultiRNNCell` with the input or output dropout like so:\n",
    "\n",
    "> \n",
    "```\n",
    "cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=global_dropout)\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=global_dropout)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Advanced -- Writing a custom RNN cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s almost too easy to use the standard GRU or LSTM cells, so let’s define our own RNN cell. Here’s a random idea that may or may not work: starting with a GRU cell, instead of taking a single transformation of its input, we enable it to take a weighted average of multiple transformations of its input. That is, using the notation from [Cho et al. (2014)](http://arxiv.org/pdf/1406.1078v3.pdf), instead of using $W_x$ in our candidate state, $h^{(t)}=\\tanh(Wx+U(r⊙h^{(t−1)}))$, we use a weighted average of $W_1x, W_2x, \\cdots, W_nx$ for some $n$. In other words, we will replace $Wx$ with $\\sum_i \\lambda_i W_ix$ for some weights $\\lambda_i$ that sum to $1$. The vector of weights, $\\lambda$, will be calculated as $\\lambda=softmax(W_{avg}x^{(t)}+U_{avg}h^{(t−1)}+b)$. The idea is that we might benefit from treat the input differently in different scenarios (e.g., we may want to treat verbs differently than nouns).\n",
    "\n",
    "To write the custom cell, we need to extend `tf.nn.rnn_cell.RNNCell`. Specifically, we need to fill in 3 abstract methods and write an `__init__` method (take a look at the Tensorflow code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py)). First, let’s start with a GRU cell, adapted from Tensorflow’s implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRUCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n",
    "\n",
    "    def __init__(self, num_units):\n",
    "        self._num_units = num_units\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n",
    "            with tf.variable_scope(\"Gates\"):  # Reset gate and update gate.\n",
    "                # We start with bias of 1.0 to not reset and not update.\n",
    "                ru = tf.nn.rnn_cell._linear([inputs, state],\n",
    "                                        2 * self._num_units, True, 1.0)\n",
    "                ru = tf.nn.sigmoid(ru)\n",
    "                r, u = tf.split(1, 2, ru)\n",
    "            with tf.variable_scope(\"Candidate\"):\n",
    "                c = tf.nn.tanh(tf.nn.rnn_cell._linear([inputs, r * state],\n",
    "                                             self._num_units, True))\n",
    "            new_h = u * state + (1 - u) * c\n",
    "        return new_h, new_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the `__init__` method to take a parameter `n` at initialization, which will determine the number of transformation matrices $W_i$ it will create:\n",
    "\n",
    "> \n",
    "```\n",
    "def __init__(self, num_units, num_weights):\n",
    "    self._num_units = num_units\n",
    "    self._num_weights = num_weights\n",
    "```\n",
    "\n",
    "Then, we modify the Candidate variable scope of the `__call__` method to do a weighted average as shown below (note that all of the Wi matrices are created as a single variable and then split into multiple tensors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/pdf/1406.1078v3.pdf).\"\"\"\n",
    "\n",
    "    def __init__(self, num_units, num_weights):\n",
    "        self._num_units = num_units\n",
    "        self._num_weights = num_weights\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        with tf.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n",
    "            with tf.variable_scope(\"Gates\"):  # Reset gate and update gate.\n",
    "                # We start with bias of 1.0 to not reset and not update.\n",
    "                ru = tf.nn.rnn_cell._linear([inputs, state],\n",
    "                                        2 * self._num_units, True, 1.0)\n",
    "                ru = tf.nn.sigmoid(ru)\n",
    "                r, u = tf.split(1, 2, ru)\n",
    "            with tf.variable_scope(\"Candidate\"):\n",
    "                lambdas = tf.nn.rnn_cell._linear([inputs, state], self._num_weights, True)\n",
    "                lambdas = tf.split(1, self._num_weights, tf.nn.softmax(lambdas))\n",
    "\n",
    "                Ws = tf.get_variable(\"Ws\",\n",
    "                        shape = [self._num_weights, inputs.get_shape()[1], self._num_units])\n",
    "                Ws = [tf.squeeze(i) for i in tf.split(0, self._num_weights, Ws)]\n",
    "\n",
    "                candidate_inputs = []\n",
    "\n",
    "                for idx, W in enumerate(Ws):\n",
    "                    candidate_inputs.append(tf.matmul(inputs, W) * lambdas[idx])\n",
    "\n",
    "                Wx = tf.add_n(candidate_inputs)\n",
    "\n",
    "                c = tf.nn.tanh(Wx + tf.nn.rnn_cell._linear([r * state],\n",
    "                                            self._num_units, True, scope=\"second\"))\n",
    "            new_h = u * state + (1 - u) * c\n",
    "        return new_h, new_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how the custom cell stacks up to a regular GRU cell (using `num_steps = 30`, since this performs much better than `num_steps = 200` after 5 epochs – can you see why that might happen?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multilayer_graph_with_custom_cell(\n",
    "    cell_type = None,\n",
    "    num_weights_for_custom_cell = 5,\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 3,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    if cell_type == 'Custom':\n",
    "        cell = CustomCell(state_size, num_weights_for_custom_cell)\n",
    "    elif cell_type == 'GRU':\n",
    "        cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    elif cell_type == 'LSTM':\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "    if cell_type == 'LSTM':\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_multilayer_graph_with_custom_cell(num_classes = vocab_size, cell_type='GRU', num_steps=30)\n",
    "t = time.time()\n",
    "train_network(g, 5, num_steps=30)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 5 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_multilayer_graph_with_custom_cell(num_classes = vocab_size, cell_type='Custom', num_steps=30)\n",
    "t = time.time()\n",
    "train_network(g, 5, num_steps=30)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 5 epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much for that idea. Our custom cell took almost twice as long to train and seems to perform worse than a standard GRU cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Advanced -- Layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normalization is a feature that was published just recently by [Lei Ba et al. (2016)](https://arxiv.org/abs/1607.06450), which we can use to improve our RNN. It was inspired by batch normalization, which you can read about and learn how to implement [here](http://r2rt.com/implementing-batch-normalization-in-tensorflow.html). Batch normalization (for feed-forward and convolutional neural networks) and layer normalization (for recurrent neural networks) generally improve training time and achieve better overall performance. In this section, we’ll apply what we’ve learned in this post to implement layer normalization in Tensorflow.\n",
    "\n",
    "Layer normalization is applied as follows: the initial layer normalization function is applied individually to each training example so as to normalize the output vector of a linear transformation to have a mean of 0 and a variance of 1.\n",
    "\n",
    "In math:\n",
    "$$\n",
    "LN_{initial}:v↦\\frac{v−\\mu_v}{\\sqrt{(σ^2_v+ϵ)}}\n",
    "$$\n",
    "for some vector $v$ and some small value of $ϵ$ for numerical stability.\n",
    "\n",
    "For some the same reasons we add scale and shift parameters to the initial batch normalization transform (see the [batch normalization post](http://r2rt.com/implementing-batch-normalization-in-tensorflow.html) for details), we add scale, $\\alpha$, and shift, $\\beta$, parameters here as well, so that the final layer normalization function is:\n",
    "$$\n",
    "LN:v↦\\alpha⊙\\frac{v−\\mu_v}{\\sqrt{(σ^2_v+ϵ)}}+\\beta\n",
    "$$\n",
    "\n",
    "Note that $⊙$ is point-wise multiplication.\n",
    "\n",
    "To add layer normalization to our network, we first write a function that will layer normalization a 2D tensor along its second dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln(tensor, scope = None, epsilon = 1e-5):\n",
    "    \"\"\" Layer normalizes a 2D tensor along its second axis \"\"\"\n",
    "    assert(len(tensor.get_shape()) == 2)\n",
    "    m, v = tf.nn.moments(tensor, [1], keep_dims=True)\n",
    "    if not isinstance(scope, str):\n",
    "        scope = ''\n",
    "    with tf.variable_scope(scope + 'layer_norm'):\n",
    "        scale = tf.get_variable('scale',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(1))\n",
    "        shift = tf.get_variable('shift',\n",
    "                                shape=[tensor.get_shape()[1]],\n",
    "                                initializer=tf.constant_initializer(0))\n",
    "    LN_initial = (tensor - m) / tf.sqrt(v + epsilon)\n",
    "\n",
    "    return LN_initial * scale + shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s apply it our layer normalization function as it was applied by [Lei Ba et al. (2016)](https://arxiv.org/abs/1607.06450) to LSTMs (in their experiments “Teaching machines to read and comprehend” and “Handwriting sequence generation”). [Lei Ba et al. (2016)](https://arxiv.org/abs/1607.06450) apply layer normalization to the output of each gate inside the LSTM cell, which means that we get to take a second shot at writing a new type of RNN cell. We’ll start with Tensorflow’s official code, located here, and modify it accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalizedLSTMCell(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"\n",
    "    Adapted from TF's BasicLSTMCell to use Layer Normalization.\n",
    "    Note that state_is_tuple is always True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, forget_bias=1.0, activation=tf.nn.tanh):\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return tf.nn.rnn_cell.LSTMStateTuple(self._num_units, self._num_units)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            c, h = state\n",
    "\n",
    "            # change bias argument to False since LN will add bias via shift\n",
    "            concat = tf.nn.rnn_cell._linear([inputs, h], 4 * self._num_units, False)\n",
    "\n",
    "            i, j, f, o = tf.split(1, 4, concat)\n",
    "\n",
    "            # add layer normalization to each gate\n",
    "            i = ln(i, scope = 'i/')\n",
    "            j = ln(j, scope = 'j/')\n",
    "            f = ln(f, scope = 'f/')\n",
    "            o = ln(o, scope = 'o/')\n",
    "\n",
    "            new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + tf.nn.sigmoid(i) *\n",
    "                   self._activation(j))\n",
    "\n",
    "            # add layer_normalization in calculation of new hidden state\n",
    "            new_h = self._activation(ln(new_c, scope = 'new_h/')) * tf.nn.sigmoid(o)\n",
    "            new_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "            return new_h, new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "    cell_type = None,\n",
    "    num_weights_for_custom_cell = 5,\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 3,\n",
    "    build_with_dropout=False,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    dropout = tf.constant(1.0)\n",
    "\n",
    "    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    if cell_type == 'Custom':\n",
    "        cell = CustomCell(state_size, num_weights_for_custom_cell)\n",
    "    elif cell_type == 'GRU':\n",
    "        cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    elif cell_type == 'LSTM':\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    elif cell_type == 'LN_LSTM':\n",
    "        cell = LayerNormalizedLSTMCell(state_size)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=dropout)\n",
    "\n",
    "    if cell_type == 'LSTM' or cell_type == 'LN_LSTM':\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = tf.matmul(rnn_outputs, W) + b\n",
    "\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        preds = predictions,\n",
    "        saver = tf.train.Saver()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compare the `GRU`, `LSTM` and `LN_LSTM` after training each for 20 epochs using 80 step sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_graph(cell_type='GRU', num_steps=80)\n",
    "t = time.time()\n",
    "losses = train_network(g, 20, num_steps=80, save=\"saves/GRU_20_epochs\")\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_graph(cell_type='LSTM', num_steps=80)\n",
    "t = time.time()\n",
    "losses = train_network(g, 20, num_steps=80, save=\"saves/LSTM_20_epochs\")\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_graph(cell_type='LN_LSTM', num_steps=80)\n",
    "t = time.time()\n",
    "losses = train_network(g, 20, num_steps=80, save=\"saves/LN_LSTM_20_epochs\")\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the layer normalized LSTM just managed to edge out the GRU in the last few epochs, though the increase in training time hardly seems worth it (perhaps my implementation could be improved?). It would be interesting to see how they would perform on a validation or test set and also to try out a layer normalized GRU. For now, let’s use the GRU to generate some text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate text, were going to rebuild the graph so as to accept a single character at a time and restore our saved model. We’ll give the network a single character prompt, grab its predicted probability distribution for the next character, use that distribution to pick the next character, and repeat. When picking the next character, our `generate_characters` function can be set to use the whole probability distribution (default), or be forced to pick one of the top n most likely characters in the distribution. The latter option should obtain more English-like results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):\n",
    "    \"\"\" Accepts a current character, initial state\"\"\"\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        g['saver'].restore(sess, checkpoint)\n",
    "\n",
    "        state = None\n",
    "        current_char = vocab_to_idx[prompt]\n",
    "        chars = [current_char]\n",
    "\n",
    "        for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                feed_dict={g['x']: [[current_char]], g['init_state']: state}\n",
    "            else:\n",
    "                feed_dict={g['x']: [[current_char]]}\n",
    "\n",
    "            preds, state = sess.run([g['preds'],g['final_state']], feed_dict)\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "\n",
    "    chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "    print(\"\".join(chars))\n",
    "    return(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_graph(cell_type='LN_LSTM', num_steps=1, batch_size=1)\n",
    "generate_characters(g, \"saves/LN_LSTM_20_epochs\", 750, prompt='A', pick_top_chars=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this network has learned something. It’s definitely not random, though there is a bit of a warm up at the beginning (the state starts at 0). I was expecting something a bit better, however, given Karpathy’s Shakespeare results. His model used more data, a state_size of 512, and was trained quite a bit longer than this one. Let’s see if we can match that. I couldn’t find a suitable premade dataset, so I had to make one myself: I concatenated the scripts from the Star Wars movies, the Star Trek movies, Tarantino and the Matrix. The final file size is 3.3MB, which is a bit smaller than the full works of William Shakespeare. Let’s load these up and try this again, with a larger state size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load new data\n",
    "\"\"\"\n",
    "\n",
    "file_url = 'https://gist.githubusercontent.com/spitis/59bfafe6966bfe60cc206ffbb760269f/'+\\\n",
    "'raw/030a08754aada17cef14eed6fac7797cda830fe8/variousscripts.txt'\n",
    "file_name = 'variousscripts.txt'\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(file_url, file_name)\n",
    "\n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))\n",
    "\n",
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_graph(cell_type='GRU',\n",
    "                num_steps=80,\n",
    "                state_size = 512,\n",
    "                batch_size = 50,\n",
    "                num_classes=vocab_size,\n",
    "                learning_rate=5e-4)\n",
    "t = time.time()\n",
    "losses = train_network(g, 30, num_steps=80, batch_size = 50, save=\"saves/GRU_30_epochs_variousscripts\")\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 30 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = build_graph(cell_type='GRU', num_steps=1, batch_size=1, num_classes=vocab_size, state_size = 512)\n",
    "generate_characters(g, \"saves/GRU_30_epochs_variousscripts\", 750, prompt='D', pick_top_chars=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1] [Understanding LSTM Networks, by Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [2] [The Unreasonable Effectiveness of Recurrent Neural Networks, by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [3] [Recurrent Neural Networks Tutorial, by Denny Britz](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "- [4] [Recurrent Neural Networks in Tensorflow, Part II, from R2RT](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
